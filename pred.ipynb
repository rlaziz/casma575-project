{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9e4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processed Training Data (d_train_processed) ---\n",
      "Original shape: (365, 16)  --> Processed shape: (365, 34)\n",
      "New columns created (example from training data):\n",
      "['season_2', 'season_3', 'season_4', 'mnth_10', 'mnth_11', 'mnth_12', 'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'holiday_1', 'weathersit_2', 'weathersit_3']\n",
      "\n",
      "--- Processed Validation Data (d_val_processed) ---\n",
      "Original shape: (366, 16)  --> Processed shape: (366, 34)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load and Split Data ---\n",
    "# (This is what you already did in your notebook)\n",
    "day_df = pd.read_csv('./data/day.csv')\n",
    "d_train = day_df[day_df['yr'] == 0].copy()\n",
    "d_val = day_df[day_df['yr'] == 1].copy()\n",
    "\n",
    "\n",
    "# --- STEP 1: Process Categorical Variables ---\n",
    "\n",
    "# Define the columns that should be treated as categories, based on your team's R code\n",
    "categorical_cols = ['season', 'mnth', 'weekday', 'holiday', 'weathersit']\n",
    "\n",
    "# To get clean column names (e.g., 'mnth_10' instead of 'mnth_10.0'),\n",
    "# we'll convert them to string type first.\n",
    "for col in categorical_cols:\n",
    "    d_train[col] = d_train[col].astype(str)\n",
    "    d_val[col] = d_val[col].astype(str)\n",
    "\n",
    "# Now, apply pd.get_dummies() to create the new columns.\n",
    "# We set drop_first=True to avoid the \"dummy variable trap\" (multicollinearity).\n",
    "# Your R 'lm()' function does this automatically in the background.\n",
    "d_train_processed = pd.get_dummies(d_train, columns=categorical_cols, drop_first=True)\n",
    "d_val_processed = pd.get_dummies(d_val, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"--- Processed Training Data (d_train_processed) ---\")\n",
    "print(f\"Original shape: {d_train.shape}  --> Processed shape: {d_train_processed.shape}\")\n",
    "print(\"New columns created (example from training data):\")\n",
    "# This list comprehension shows only the *new* columns you just made\n",
    "new_cols = [col for col in d_train_processed.columns if col not in day_df.columns]\n",
    "print(new_cols)\n",
    "\n",
    "print(\"\\n--- Processed Validation Data (d_val_processed) ---\")\n",
    "print(f\"Original shape: {d_val.shape}  --> Processed shape: {d_val_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380e208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- X_train (Predictors for Training) ---\n",
      "Shape: (365, 26)\n",
      "First 5 rows:\n",
      "      atemp       hum  windspeed  season_2  season_3  season_4  mnth_10  \\\n",
      "0  0.363625  0.805833   0.160446     False     False     False    False   \n",
      "1  0.353739  0.696087   0.248539     False     False     False    False   \n",
      "2  0.189405  0.437273   0.248309     False     False     False    False   \n",
      "3  0.212122  0.590435   0.160296     False     False     False    False   \n",
      "4  0.229270  0.436957   0.186900     False     False     False    False   \n",
      "\n",
      "   mnth_11  mnth_12  mnth_2  ...  mnth_9  weekday_1  weekday_2  weekday_3  \\\n",
      "0    False    False   False  ...   False      False      False      False   \n",
      "1    False    False   False  ...   False      False      False      False   \n",
      "2    False    False   False  ...   False       True      False      False   \n",
      "3    False    False   False  ...   False      False       True      False   \n",
      "4    False    False   False  ...   False      False      False       True   \n",
      "\n",
      "   weekday_4  weekday_5  weekday_6  holiday_1  weathersit_2  weathersit_3  \n",
      "0      False      False       True      False          True         False  \n",
      "1      False      False      False      False          True         False  \n",
      "2      False      False      False      False         False         False  \n",
      "3      False      False      False      False         False         False  \n",
      "4      False      False      False      False         False         False  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "--- y_train (Target for Training) ---\n",
      "Shape: (365,)\n",
      "First 5 rows:\n",
      "0     985\n",
      "1     801\n",
      "2    1349\n",
      "3    1562\n",
      "4    1600\n",
      "Name: cnt, dtype: int64\n",
      "\n",
      "Total number of features: 26\n",
      "List of features being used:\n",
      "['atemp', 'hum', 'windspeed', 'season_2', 'season_3', 'season_4', 'mnth_10', 'mnth_11', 'mnth_12', 'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'holiday_1', 'weathersit_2', 'weathersit_3']\n"
     ]
    }
   ],
   "source": [
    "# --- We'll use the dataframes from Step 1 ---\n",
    "# d_train_processed\n",
    "# d_val_processed\n",
    "\n",
    "# --- STEP 2: Define 'X' (Predictors) and 'y' (Target) ---\n",
    "\n",
    "# 1. Define our continuous predictor variables (from your R code)\n",
    "continuous_cols = ['atemp', 'hum', 'windspeed']\n",
    "\n",
    "# 2. Get the list of the new dummy columns we created in Step 1\n",
    "#    These are all the columns in d_train_processed that are not\n",
    "#    the original columns, or the target ('cnt').\n",
    "original_cols = list(d_train.columns)\n",
    "target_col = 'cnt'\n",
    "# This list comprehension gets all columns that are \"new\"\n",
    "dummy_cols = [col for col in d_train_processed.columns\n",
    "              if col not in original_cols and col != target_col]\n",
    "\n",
    "# 3. Combine the lists to create our final list of features\n",
    "features = continuous_cols + dummy_cols\n",
    "\n",
    "# 4. Define the target variable (what we want to predict)\n",
    "target = 'cnt'\n",
    "\n",
    "\n",
    "# --- Create the X and y dataframes ---\n",
    "\n",
    "# For Training (2011 data)\n",
    "X_train = d_train_processed[features]\n",
    "y_train = d_train_processed[target]\n",
    "\n",
    "# For Validation (2012 data)\n",
    "# IMPORTANT: Use the *same* 'features' list to select columns\n",
    "X_val = d_val_processed[features]\n",
    "y_val = d_val_processed[target]\n",
    "\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"--- X_train (Predictors for Training) ---\")\n",
    "print(f\"Shape: {X_train.shape}\")\n",
    "print(\"First 5 rows:\")\n",
    "print(X_train.head())\n",
    "\n",
    "print(\"\\n--- y_train (Target for Training) ---\")\n",
    "print(f\"Shape: {y_train.shape}\")\n",
    "print(\"First 5 rows:\")\n",
    "print(y_train.head())\n",
    "\n",
    "print(f\"\\nTotal number of features: {len(features)}\")\n",
    "print(\"List of features being used:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cfb0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Model (Linear Regression) ---\n",
      "Model has been successfully trained on the 2011 (yr==0) data.\n",
      "Training R-squared (R2): 0.8397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- STEP 3: Build and \"Fit\" Your Baseline Model ---\n",
    "\n",
    "# 1. Create an instance of the Linear Regression model\n",
    "#    We'll call it 'baseline_model'\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# 2. \"Fit\" the model to your 2011 training data\n",
    "#    This is where the model \"learns\" from your X_train and y_train\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"--- Baseline Model (Linear Regression) ---\")\n",
    "print(\"Model has been successfully trained on the 2011 (yr==0) data.\")\n",
    "\n",
    "# We can even look at the R-squared score on the *training* data\n",
    "# This tells us how well the model fits the data it learned from.\n",
    "train_r2 = baseline_model.score(X_train, y_train)\n",
    "print(f\"Training R-squared (R2): {train_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d8b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Model Performance (on 2012 Validation Data) ---\n",
      "Validation R-squared (R2): -0.6624\n",
      "Validation MAPE: 63.62%\n",
      "\n",
      "--- Actual vs. Predicted (First 10 rows) ---\n",
      "     Actual_Cnt  Predicted_Cnt\n",
      "365        2294    1733.897528\n",
      "366        1951    1242.215998\n",
      "367        2236    1086.804749\n",
      "368        2368    1100.503833\n",
      "369        3272    1832.547815\n",
      "370        4098    2007.706849\n",
      "371        4521    2166.959541\n",
      "372        3425    1919.582261\n",
      "373        2376    1287.384903\n",
      "374        3598    1762.948927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# --- STEP 4: Test Your Baseline Model (on 2012 Validation Data) ---\n",
    "\n",
    "# 1. Use the *trained* 'baseline_model' to make predictions on the 2012 data (X_val)\n",
    "y_pred_baseline = baseline_model.predict(X_val)\n",
    "\n",
    "# 2. Calculate the performance metrics\n",
    "# We compare the 'y_val' (the *actual* 2012 counts) with 'y_pred_baseline'\n",
    "validation_r2 = r2_score(y_val, y_pred_baseline)\n",
    "validation_mape = mean_absolute_percentage_error(y_val, y_pred_baseline)\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"--- Baseline Model Performance (on 2012 Validation Data) ---\")\n",
    "print(f\"Validation R-squared (R2): {validation_r2:.4f}\")\n",
    "print(f\"Validation MAPE: {validation_mape * 100:.2f}%\")\n",
    "\n",
    "# Let's also create a small DataFrame to see the predictions vs. actuals\n",
    "print(\"\\n--- Actual vs. Predicted (First 10 rows) ---\")\n",
    "comparison_df = pd.DataFrame({'Actual_Cnt': y_val, 'Predicted_Cnt': y_pred_baseline})\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba190c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'Aha!' Model Performance (with atemp_squared) ---\n",
      "Validation R-squared (R2): -0.5942\n",
      "Validation MAPE: 63.09%\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE: 63.62%\n",
      "New 'Aha' Model MAPE: 63.09%\n"
     ]
    }
   ],
   "source": [
    "# --- We'll use the dataframes from Step 1 & 2 ---\n",
    "# d_train_processed, d_val_processed\n",
    "# features, y_train, y_val\n",
    "\n",
    "# --- STEP 5: Add the 'atemp_squared' Feature ---\n",
    "\n",
    "# 1. Create the new 'atemp_squared' feature\n",
    "#    This is simply 'atemp' * 'atemp'.\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "\n",
    "# 2. Define a *new* list of features, including the new column\n",
    "features_poly = features + ['atemp_squared']\n",
    "\n",
    "# 3. Create new X_train and X_val with this new feature\n",
    "X_train_poly = d_train_processed[features_poly]\n",
    "X_val_poly = d_val_processed[features_poly]\n",
    "\n",
    "# 4. Create and fit the new \"Polynomial\" Model\n",
    "polynomial_model = LinearRegression()\n",
    "polynomial_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# 5. Test the new model on the 2012 validation data\n",
    "y_pred_poly = polynomial_model.predict(X_val_poly)\n",
    "\n",
    "# 6. Calculate the new performance metrics\n",
    "poly_r2 = r2_score(y_val, y_pred_poly)\n",
    "poly_mape = mean_absolute_percentage_error(y_val, y_pred_poly)\n",
    "\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"--- 'Aha!' Model Performance (with atemp_squared) ---\")\n",
    "print(f\"Validation R-squared (R2): {poly_r2:.4f}\")\n",
    "print(f\"Validation MAPE: {poly_mape * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "# (baseline_mape was calculated in my previous run's code,\n",
    "#  but we'll just show the numbers for clarity)\n",
    "print(f\"Baseline Model MAPE: 63.62%\")\n",
    "print(f\"New 'Aha' Model MAPE: {poly_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90d968b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset as it appears to not be loaded yet in this cell\n",
    "data = pd.read_csv('data/day.csv')\n",
    "\n",
    "# Plot the distribution of \"temp\"\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(data['temp'], bins=30, color='salmon', edgecolor='black')\n",
    "plt.title('Distribution of Normalized Temperature (temp)')\n",
    "plt.xlabel('temp')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7301e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Two Models (Weekday/Weekend) Have Been Trained ---\n",
      "Original y_val length: 366\n",
      "Combined y_val length: 366\n",
      "\n",
      "--- 'Two Model' Performance ---\n",
      "Validation R-squared (R2): -0.6659\n",
      "Validation MAPE: 65.27%\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "New 'Two Model' MAPE: 65.27%\n"
     ]
    }
   ],
   "source": [
    "# --- We'll use the dataframes from Step 1 ---\n",
    "# d_train_processed, d_val_processed\n",
    "# And the 'features' list from Step 2, and y_train/y_val\n",
    "\n",
    "# --- STEP 6: Build Separate Weekday/Weekend Models ---\n",
    "# This is based on your insight and your team's 'Report-3.R' file\n",
    "\n",
    "# 1. Create the new training/validation sets\n",
    "# Your 'day.csv' has a 'workingday' column.\n",
    "# 1 = weekday (Mon-Fri, not a holiday)\n",
    "# 0 = weekend or holiday\n",
    "# This is the easiest way to split the data.\n",
    "\n",
    "# Split the 2011 TRAINING data\n",
    "X_train_wkday = d_train_processed[d_train_processed['workingday'] == 1][features]\n",
    "y_train_wkday = d_train_processed[d_train_processed['workingday'] == 1][target]\n",
    "\n",
    "X_train_wkend = d_train_processed[d_train_processed['workingday'] == 0][features]\n",
    "y_train_wkend = d_train_processed[d_train_processed['workingday'] == 0][target]\n",
    "\n",
    "# Split the 2012 VALIDATION data\n",
    "X_val_wkday = d_val_processed[d_val_processed['workingday'] == 1][features]\n",
    "y_val_wkday = d_val_processed[d_val_processed['workingday'] == 1][target]\n",
    "\n",
    "X_val_wkend = d_val_processed[d_val_processed['workingday'] == 0][features]\n",
    "y_val_wkend = d_val_processed[d_val_processed['workingday'] == 0][target]\n",
    "\n",
    "\n",
    "# 2. Train TWO separate models\n",
    "model_wkday = LinearRegression()\n",
    "model_wkday.fit(X_train_wkday, y_train_wkday)\n",
    "\n",
    "model_wkend = LinearRegression()\n",
    "model_wkend.fit(X_train_wkend, y_train_wkend)\n",
    "\n",
    "print(\"--- Two Models (Weekday/Weekend) Have Been Trained ---\")\n",
    "\n",
    "\n",
    "# 3. Make predictions on the corresponding 2012 validation sets\n",
    "preds_wkday = model_wkday.predict(X_val_wkday)\n",
    "preds_wkend = model_wkend.predict(X_val_wkend)\n",
    "\n",
    "# 4. Combine all predictions and all actuals\n",
    "# We need to put them back in the right order to test\n",
    "# We can do this by combining the y_val and prediction arrays\n",
    "y_val_combined = np.concatenate([y_val_wkday, y_val_wkend])\n",
    "preds_combined = np.concatenate([preds_wkday, preds_wkend])\n",
    "\n",
    "# 5. Calculate the FINAL performance\n",
    "# NOTE: This only works if our combined arrays have the same\n",
    "#       number of elements as the original y_val (366).\n",
    "#       Let's check.\n",
    "print(f\"Original y_val length: {len(y_val)}\")\n",
    "print(f\"Combined y_val length: {len(y_val_combined)}\")\n",
    "# The lengths should match! (366)\n",
    "\n",
    "final_r2 = r2_score(y_val_combined, preds_combined)\n",
    "final_mape = mean_absolute_percentage_error(y_val_combined, preds_combined)\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- 'Two Model' Performance ---\")\n",
    "print(f\"Validation R-squared (R2): {final_r2:.4f}\")\n",
    "print(f\"Validation MAPE: {final_mape * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"New 'Two Model' MAPE: {final_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59be47f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training the Random Forest Model (this may take a moment) ---\n",
      "--- Random Forest Model Trained ---\n",
      "\n",
      "--- 'Pro' Model Performance (Random Forest) ---\n",
      "Validation R-squared (R2): -0.5231\n",
      "Validation MAPE: 64.73%\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (Linear Regression): 63.62%\n",
      "New 'Pro' Model MAPE (Random Forest): 64.73%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# --- STEP 7: Try a \"Pro\" Model (Random Forest) ---\n",
    "# We'll use the original X_train/y_train from Step 2\n",
    "\n",
    "# 1. Create an instance of the Random Forest model\n",
    "#    n_estimators=100 means it will build 100 decision trees.\n",
    "#    random_state=42 makes sure you get the same result every time.\n",
    "#    n_jobs=-1 uses all your computer's cores to speed it up.\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 2. \"Fit\" the model to your 2011 training data\n",
    "print(\"--- Training the Random Forest Model (this may take a moment) ---\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"--- Random Forest Model Trained ---\")\n",
    "\n",
    "# 3. Test the new model on the 2012 validation data\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# 4. Calculate the new performance metrics\n",
    "rf_r2 = r2_score(y_val, y_pred_rf)\n",
    "rf_mape = mean_absolute_percentage_error(y_val, y_pred_rf)\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- 'Pro' Model Performance (Random Forest) ---\")\n",
    "print(f\"Validation R-squared (R2): {rf_r2:.4f}\")\n",
    "print(f\"Validation MAPE: {rf_mape * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (Linear Regression): 63.62%\")\n",
    "print(f\"New 'Pro' Model MAPE (Random Forest): {rf_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659a4193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Two Models (Weekday/Weekend) Have Been Trained ---\n",
      "\n",
      "--- 'Two Model' Performance (Correctly Scored) ---\n",
      "Validation R-squared (R2): -0.6659\n",
      "Validation MAPE: 65.27%\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "New 'Two Model' MAPE: 65.27%\n"
     ]
    }
   ],
   "source": [
    "# --- We'll use the dataframes from Step 1 ---\n",
    "# d_train_processed, d_val_processed\n",
    "# And the 'features' list from Step 2, and y_train/y_val\n",
    "\n",
    "# --- STEP 7 (Redux): Re-run \"Two Model\" Approach *Correctly* ---\n",
    "\n",
    "# 1. Get the 2011 TRAINING data\n",
    "X_train_wkday = d_train_processed[d_train_processed['workingday'] == 1][features]\n",
    "y_train_wkday = d_train_processed[d_train_processed['workingday'] == 1][target]\n",
    "\n",
    "X_train_wkend = d_train_processed[d_train_processed['workingday'] == 0][features]\n",
    "y_train_wkend = d_train_processed[d_train_processed['workingday'] == 0][target]\n",
    "\n",
    "# 2. Get the 2012 VALIDATION data\n",
    "X_val_wkday = d_val_processed[d_val_processed['workingday'] == 1][features]\n",
    "y_val_wkday = d_val_processed[d_val_processed['workingday'] == 1][target]\n",
    "\n",
    "X_val_wkend = d_val_processed[d_val_processed['workingday'] == 0][features]\n",
    "y_val_wkend = d_val_processed[d_val_processed['workingday'] == 0][target]\n",
    "\n",
    "# 3. Train the TWO separate models\n",
    "model_wkday = LinearRegression()\n",
    "model_wkday.fit(X_train_wkday, y_train_wkday)\n",
    "\n",
    "model_wkend = LinearRegression()\n",
    "model_wkend.fit(X_train_wkend, y_train_wkend)\n",
    "\n",
    "print(\"--- Two Models (Weekday/Weekend) Have Been Trained ---\")\n",
    "\n",
    "# 4. Make predictions on the 2012 validation sets\n",
    "preds_wkday = model_wkday.predict(X_val_wkday)\n",
    "preds_wkend = model_wkend.predict(X_val_wkend)\n",
    "\n",
    "# 5. --- THIS IS THE CRITICAL FIX ---\n",
    "#    Create a new DataFrame to hold our ordered predictions.\n",
    "#    We use the *original* y_val as the template.\n",
    "final_preds_df = pd.DataFrame(index=y_val.index, columns=['predicted_cnt'])\n",
    "\n",
    "#    Now, place the predictions in the *correct* index locations\n",
    "final_preds_df.loc[y_val_wkday.index, 'predicted_cnt'] = preds_wkday\n",
    "final_preds_df.loc[y_val_wkend.index, 'predicted_cnt'] = preds_wkend\n",
    "\n",
    "# 6. Calculate the FINAL performance\n",
    "#    We compare the *original* y_val to our new, *correctly ordered* predictions\n",
    "final_r2 = r2_score(y_val, final_preds_df['predicted_cnt'])\n",
    "final_mape = mean_absolute_percentage_error(y_val, final_preds_df['predicted_cnt'])\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- 'Two Model' Performance (Correctly Scored) ---\")\n",
    "print(f\"Validation R-squared (R2): {final_r2:.4f}\")\n",
    "print(f\"Validation MAPE: {final_mape * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"New 'Two Model' MAPE: {final_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f84269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New 'Engineered' Feature Set ---\n",
      "Total features to select from: 29\n",
      "New features added:\n",
      "  atemp_squared\n",
      "  workingday\n",
      "  atemp_x_workingday\n",
      "\n",
      "MAPE for 'Full Engineered' Model: 61.89%\n",
      "Baseline Model MAPE: 63.62%\n"
     ]
    }
   ],
   "source": [
    "# --- We'll use the dataframes from Step 1 ---\n",
    "# d_train_processed, d_val_processed\n",
    "# And the 'features' list from Step 2\n",
    "\n",
    "# --- STEP 8: Engineer \"Psychology\" Features ---\n",
    "\n",
    "# 1. Add 'atemp_squared' (for \"too hot/too cold\")\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "\n",
    "# 2. Add the interaction term (for \"bimodal\" weekday/weekend)\n",
    "#    We need the 'workingday' column first.\n",
    "#    Let's grab it from the *original* d_train/d_val.\n",
    "d_train_processed['workingday'] = d_train['workingday'].astype(int)\n",
    "d_val_processed['workingday'] = d_val['workingday'].astype(int)\n",
    "\n",
    "# This is the interaction feature\n",
    "d_train_processed['atemp_x_workingday'] = d_train_processed['atemp'] * d_train_processed['workingday']\n",
    "d_val_processed['atemp_x_workingday'] = d_val_processed['atemp'] * d_val_processed['workingday']\n",
    "\n",
    "# 3. Create our *new*, *full* universe of features\n",
    "#    This includes the original 26 + our 2 new engineered ones + 'workingday'\n",
    "#    (We need 'workingday' in the model for the interaction to make sense)\n",
    "features_engineered = features + ['atemp_squared', 'workingday', 'atemp_x_workingday']\n",
    "\n",
    "# 4. Create the new X and y for this full model\n",
    "X_train_eng = d_train_processed[features_engineered]\n",
    "y_train_eng = d_train_processed[target]\n",
    "\n",
    "# We must .align to make sure X_val has all the same columns in the same order\n",
    "X_val_eng, _ = d_val_processed.align(X_train_eng, join='right', axis=1, fill_value=0)\n",
    "X_val_eng = X_val_eng[features_engineered] # Re-order columns to match\n",
    "y_val_eng = d_val_processed[target]\n",
    "\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"--- New 'Engineered' Feature Set ---\")\n",
    "print(f\"Total features to select from: {len(features_engineered)}\")\n",
    "print(\"New features added:\")\n",
    "print(\"  atemp_squared\")\n",
    "print(\"  workingday\")\n",
    "print(\"  atemp_x_workingday\")\n",
    "\n",
    "# Let's quickly test this \"full\" model\n",
    "full_eng_model = LinearRegression()\n",
    "full_eng_model.fit(X_train_eng, y_train_eng)\n",
    "\n",
    "y_pred_eng = full_eng_model.predict(X_val_eng)\n",
    "full_eng_mape = mean_absolute_percentage_error(y_val_eng, y_pred_eng)\n",
    "\n",
    "print(f\"\\nMAPE for 'Full Engineered' Model: {full_eng_mape * 100:.2f}%\")\n",
    "print(f\"Baseline Model MAPE: 63.62%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec76db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 'Tedious' Forward Selection ---\n",
      "Testing combinations of 29 features...\n",
      "Starting MAPE (predicting the mean): 87.31%\n",
      "  Added 'weathersit_3'. New Best MAPE: 63.77%\n",
      "  Added 'windspeed'. New Best MAPE: 55.79%\n",
      "  Added 'season_3'. New Best MAPE: 52.13%\n",
      "  Added 'season_2'. New Best MAPE: 47.41%\n",
      "  Added 'mnth_11'. New Best MAPE: 44.28%\n",
      "  Added 'mnth_9'. New Best MAPE: 43.63%\n",
      "  Added 'hum'. New Best MAPE: 42.64%\n",
      "  Added 'mnth_12'. New Best MAPE: 42.18%\n",
      "  Added 'holiday_1'. New Best MAPE: 41.99%\n",
      "  Added 'weekday_2'. New Best MAPE: 41.88%\n",
      "  Added 'weekday_6'. New Best MAPE: 41.82%\n",
      "  Added 'weekday_5'. New Best MAPE: 41.77%\n",
      "  Added 'weekday_4'. New Best MAPE: 41.69%\n",
      "  Added 'mnth_8'. New Best MAPE: 41.67%\n",
      "\n",
      "--- Selection Complete ---\n",
      "No more features improve the model.\n",
      "\n",
      "--- Final 'Tedious Selection' Model ---\n",
      "Final Best MAPE: 41.67%\n",
      "Number of features selected: 14\n",
      "\n",
      "Final list of selected features:\n",
      "['weathersit_3', 'windspeed', 'season_3', 'season_2', 'mnth_11', 'mnth_9', 'hum', 'mnth_12', 'holiday_1', 'weekday_2', 'weekday_6', 'weekday_5', 'weekday_4', 'mnth_8']\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "Full Engineered Model MAPE (All 29 features): 61.89%\n",
      "Final 'Tedious' Model MAPE: 41.67%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import warnings\n",
    "\n",
    "# --- Re-run All Previous Steps to Build Engineered Data ---\n",
    "\n",
    "# Suppress warnings for a clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and split\n",
    "day_df = pd.read_csv('./data/day.csv')\n",
    "d_train = day_df[day_df['yr'] == 0].copy()\n",
    "d_val = day_df[day_df['yr'] == 1].copy()\n",
    "target = 'cnt'\n",
    "\n",
    "# Step 1: Process Categorical Variables\n",
    "categorical_cols = ['season', 'mnth', 'weekday', 'holiday', 'weathersit']\n",
    "for col in categorical_cols:\n",
    "    d_train[col] = d_train[col].astype(str)\n",
    "    d_val[col] = d_val[col].astype(str)\n",
    "d_train_processed = pd.get_dummies(d_train, columns=categorical_cols, drop_first=True)\n",
    "d_val_processed = pd.get_dummies(d_val, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Step 2: Define base features\n",
    "continuous_cols = ['atemp', 'hum', 'windspeed']\n",
    "original_cols = list(d_train.columns)\n",
    "dummy_cols = [col for col in d_train_processed.columns if col not in original_cols and col != target]\n",
    "features = continuous_cols + dummy_cols\n",
    "\n",
    "# Step 8: Engineer \"Psychology\" Features\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "d_train_processed['workingday'] = d_train['workingday'].astype(int)\n",
    "d_val_processed['workingday'] = d_val['workingday'].astype(int)\n",
    "d_train_processed['atemp_x_workingday'] = d_train_processed['atemp'] * d_train_processed['workingday']\n",
    "d_val_processed['atemp_x_workingday'] = d_val_processed['atemp'] * d_val_processed['workingday']\n",
    "\n",
    "# Create the *full* universe of features to select from\n",
    "features_engineered = features + ['atemp_squared', 'workingday', 'atemp_x_workingday']\n",
    "\n",
    "# Create the final X and y for training and validation\n",
    "y_train_eng = d_train_processed[target]\n",
    "y_val_eng = d_val_processed[target]\n",
    "\n",
    "# --- This is a critical fix to ensure columns align ---\n",
    "# We must .align to make sure X_train and X_val have the *exact* same columns\n",
    "# in the *exact* same order.\n",
    "X_train_full, X_val_full = d_train_processed.align(d_val_processed, join='inner', axis=1, fill_value=0)\n",
    "X_train_eng = X_train_full[features_engineered]\n",
    "X_val_eng = X_val_full[features_engineered]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# --- STEP 9: The \"Tedious\" Automated Selector (Forward Selection) ---\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"--- Starting 'Tedious' Forward Selection ---\")\n",
    "print(f\"Testing combinations of {len(features_engineered)} features...\")\n",
    "\n",
    "# Start with a \"null\" model (just predicting the average) to get a baseline MAPE\n",
    "y_pred_null = np.full_like(y_val_eng, y_train_eng.mean())\n",
    "current_mape = mean_absolute_percentage_error(y_val_eng, y_pred_null)\n",
    "print(f\"Starting MAPE (predicting the mean): {current_mape * 100:.2f}%\")\n",
    "\n",
    "selected_features = []\n",
    "remaining_features = features_engineered.copy()\n",
    "\n",
    "# This loop will continue as long as we find improvements\n",
    "while True:\n",
    "    best_new_mape = current_mape\n",
    "    best_new_feature = None\n",
    "\n",
    "    # Test adding each remaining feature\n",
    "    for feature in remaining_features:\n",
    "        # Create a temporary list of features to test\n",
    "        temp_features = selected_features + [feature]\n",
    "        \n",
    "        # Create and fit the model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_eng[temp_features], y_train_eng)\n",
    "        \n",
    "        # Test on validation data\n",
    "        y_pred_temp = model.predict(X_val_eng[temp_features])\n",
    "        \n",
    "        # Calculate MAPE\n",
    "        test_mape = mean_absolute_percentage_error(y_val_eng, y_pred_temp)\n",
    "        \n",
    "        # Check if this is the best feature we've seen *so far*\n",
    "        if test_mape < best_new_mape:\n",
    "            best_new_mape = test_mape\n",
    "            best_new_feature = feature\n",
    "\n",
    "    # After testing all remaining features, check if we found a winner\n",
    "    if best_new_feature is not None:\n",
    "        # We found a feature that helps!\n",
    "        # Add it to our selected list\n",
    "        selected_features.append(best_new_feature)\n",
    "        # Remove it from the \"to-do\" list\n",
    "        remaining_features.remove(best_new_feature)\n",
    "        # Our new \"best\" MAPE is now the one with this feature\n",
    "        current_mape = best_new_mape\n",
    "        \n",
    "        print(f\"  Added '{best_new_feature}'. New Best MAPE: {current_mape * 100:.2f}%\")\n",
    "        \n",
    "    else:\n",
    "        # If we went through all remaining features and none of them\n",
    "        # improved the MAPE, we are done!\n",
    "        print(\"\\n--- Selection Complete ---\")\n",
    "        print(\"No more features improve the model.\")\n",
    "        break\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- Final 'Tedious Selection' Model ---\")\n",
    "print(f\"Final Best MAPE: {current_mape * 100:.2f}%\")\n",
    "print(f\"Number of features selected: {len(selected_features)}\")\n",
    "print(\"\\nFinal list of selected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"Full Engineered Model MAPE (All 29 features): 61.89%\")\n",
    "print(f\"Final 'Tedious' Model MAPE: {current_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "372a2150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'Master Universe' Feature Set ---\n",
      "Total features to select from: 33\n",
      "\n",
      "New cyclical features added:\n",
      "['mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n",
      "\n",
      "--- Sanity Check: First 5 rows of new features (Training Data) ---\n",
      "   mnth_int  mnth_sin  mnth_cos  weekday_int  weekday_sin  weekday_cos\n",
      "0         1       0.5  0.866025            6    -0.781831     0.623490\n",
      "1         1       0.5  0.866025            0     0.000000     1.000000\n",
      "2         1       0.5  0.866025            1     0.781831     0.623490\n",
      "3         1       0.5  0.866025            2     0.974928    -0.222521\n",
      "4         1       0.5  0.866025            3     0.433884    -0.900969\n"
     ]
    }
   ],
   "source": [
    "# --- We'll use the dataframes from Step 8 ---\n",
    "# d_train_processed, d_val_processed\n",
    "# And the 'features_engineered' list\n",
    "\n",
    "# --- STEP 10: Create Cyclical (sin/cos) Features ---\n",
    "\n",
    "# We need the original 'mnth' and 'weekday' columns as integers.\n",
    "# Let's grab them from the *original* d_train/d_val data.\n",
    "d_train_processed['mnth_int'] = d_train['mnth'].astype(int)\n",
    "d_val_processed['mnth_int'] = d_val['mnth'].astype(int)\n",
    "\n",
    "d_train_processed['weekday_int'] = d_train['weekday'].astype(int)\n",
    "d_val_processed['weekday_int'] = d_val['weekday'].astype(int)\n",
    "\n",
    "\n",
    "# 1. Create 'mnth' cyclical features (1-12)\n",
    "d_train_processed['mnth_sin'] = np.sin(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_train_processed['mnth_cos'] = np.cos(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_sin'] = np.sin(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_cos'] = np.cos(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "\n",
    "# 2. Create 'weekday' cyclical features (0-6, where Sunday=0)\n",
    "d_train_processed['weekday_sin'] = np.sin(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_train_processed['weekday_cos'] = np.cos(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_sin'] = np.sin(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_cos'] = np.cos(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "\n",
    "# 3. Create our new \"Master Universe\" of features\n",
    "#    (features_engineered = the 29 features from Step 8)\n",
    "cyclical_features = ['mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n",
    "features_master_universe = features_engineered + cyclical_features\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"--- 'Master Universe' Feature Set ---\")\n",
    "print(f\"Total features to select from: {len(features_master_universe)}\")\n",
    "print(\"\\nNew cyclical features added:\")\n",
    "print(cyclical_features)\n",
    "\n",
    "print(\"\\n--- Sanity Check: First 5 rows of new features (Training Data) ---\")\n",
    "print(d_train_processed[['mnth_int', 'mnth_sin', 'mnth_cos', 'weekday_int', 'weekday_sin', 'weekday_cos']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "748f6f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting FINAL 'Tedious' Forward Selection ---\n",
      "Testing combinations of 33 features...\n",
      "Starting MAPE (predicting the mean): 87.31%\n",
      "  Added 'weathersit_3'. New Best MAPE: 63.77%\n",
      "  Added 'windspeed'. New Best MAPE: 55.79%\n",
      "  Added 'mnth_cos'. New Best MAPE: 51.36%\n",
      "  Added 'mnth_11'. New Best MAPE: 48.21%\n",
      "  Added 'mnth_9'. New Best MAPE: 46.38%\n",
      "  Added 'mnth_12'. New Best MAPE: 44.83%\n",
      "  Added 'hum'. New Best MAPE: 43.29%\n",
      "  Added 'weekday_2'. New Best MAPE: 43.12%\n",
      "  Added 'holiday_1'. New Best MAPE: 43.03%\n",
      "  Added 'weekday_6'. New Best MAPE: 42.97%\n",
      "  Added 'mnth_8'. New Best MAPE: 42.95%\n",
      "  Added 'mnth_5'. New Best MAPE: 42.91%\n",
      "  Added 'weekday_4'. New Best MAPE: 42.89%\n",
      "  Added 'weekday_5'. New Best MAPE: 42.88%\n",
      "\n",
      "--- Selection Complete ---\n",
      "No more features improve the model.\n",
      "\n",
      "--- FINAL MODEL (Master Selection) ---\n",
      "Final Best MAPE: 42.88%\n",
      "Number of features selected: 14\n",
      "\n",
      "Final list of selected features:\n",
      "['weathersit_3', 'windspeed', 'mnth_cos', 'mnth_11', 'mnth_9', 'mnth_12', 'hum', 'weekday_2', 'holiday_1', 'weekday_6', 'mnth_8', 'mnth_5', 'weekday_4', 'weekday_5']\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "Selector Model 1 MAPE (Dummies Only): 41.67%\n",
      "FINAL MODEL MAPE (Master Universe): 42.88%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import warnings\n",
    "\n",
    "# --- Re-run All Previous Steps to Build Master Data ---\n",
    "\n",
    "# Suppress warnings for a clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and split\n",
    "day_df = pd.read_csv('./data/day.csv')\n",
    "d_train = day_df[day_df['yr'] == 0].copy()\n",
    "d_val = day_df[day_df['yr'] == 1].copy()\n",
    "target = 'cnt'\n",
    "\n",
    "# Step 1: Process Categorical Variables\n",
    "categorical_cols = ['season', 'mnth', 'weekday', 'holiday', 'weathersit']\n",
    "for col in categorical_cols:\n",
    "    d_train[col] = d_train[col].astype(str)\n",
    "    d_val[col] = d_val[col].astype(str)\n",
    "d_train_processed = pd.get_dummies(d_train, columns=categorical_cols, drop_first=True)\n",
    "d_val_processed = pd.get_dummies(d_val, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Step 2: Define base features\n",
    "continuous_cols = ['atemp', 'hum', 'windspeed']\n",
    "original_cols = list(d_train.columns)\n",
    "dummy_cols = [col for col in d_train_processed.columns if col not in original_cols and col != target]\n",
    "features = continuous_cols + dummy_cols\n",
    "\n",
    "# Step 8: Engineer \"Psychology\" Features\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "d_train_processed['workingday'] = d_train['workingday'].astype(int)\n",
    "d_val_processed['workingday'] = d_val['workingday'].astype(int)\n",
    "d_train_processed['atemp_x_workingday'] = d_train_processed['atemp'] * d_train_processed['workingday']\n",
    "d_val_processed['atemp_x_workingday'] = d_val_processed['atemp'] * d_val_processed['workingday']\n",
    "features_engineered = features + ['atemp_squared', 'workingday', 'atemp_x_workingday']\n",
    "\n",
    "# Step 10: Create Cyclical Features\n",
    "d_train_processed['mnth_int'] = d_train['mnth'].astype(int)\n",
    "d_val_processed['mnth_int'] = d_val['mnth'].astype(int)\n",
    "d_train_processed['weekday_int'] = d_train['weekday'].astype(int)\n",
    "d_val_processed['weekday_int'] = d_val['weekday'].astype(int)\n",
    "d_train_processed['mnth_sin'] = np.sin(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_train_processed['mnth_cos'] = np.cos(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_sin'] = np.sin(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_cos'] = np.cos(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_train_processed['weekday_sin'] = np.sin(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_train_processed['weekday_cos'] = np.cos(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_sin'] = np.sin(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_cos'] = np.cos(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "\n",
    "# --- THIS IS THE NEW LIST ---\n",
    "cyclical_features = ['mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n",
    "features_master_universe = features_engineered + cyclical_features\n",
    "# ----------------------------\n",
    "\n",
    "# Create the final X and y for training and validation\n",
    "y_train_master = d_train_processed[target]\n",
    "y_val_master = d_val_processed[target]\n",
    "\n",
    "# --- This is a critical fix to ensure columns align ---\n",
    "X_train_full, X_val_full = d_train_processed.align(d_val_processed, join='inner', axis=1, fill_value=0)\n",
    "X_train_master = X_train_full[features_master_universe]\n",
    "X_val_master = X_val_full[features_master_universe]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# --- STEP 11: The \"Tedious\" Selector on the \"Master Universe\" ---\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"--- Starting FINAL 'Tedious' Forward Selection ---\")\n",
    "print(f\"Testing combinations of {len(features_master_universe)} features...\")\n",
    "\n",
    "# Start with a \"null\" model\n",
    "y_pred_null = np.full_like(y_val_master, y_train_master.mean())\n",
    "current_mape = mean_absolute_percentage_error(y_val_master, y_pred_null)\n",
    "print(f\"Starting MAPE (predicting the mean): {current_mape * 100:.2f}%\")\n",
    "\n",
    "selected_features = []\n",
    "remaining_features = features_master_universe.copy()\n",
    "\n",
    "while True:\n",
    "    best_new_mape = current_mape\n",
    "    best_new_feature = None\n",
    "\n",
    "    # Test adding each remaining feature\n",
    "    for feature in remaining_features:\n",
    "        temp_features = selected_features + [feature]\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_master[temp_features], y_train_master)\n",
    "        \n",
    "        y_pred_temp = model.predict(X_val_master[temp_features])\n",
    "        \n",
    "        test_mape = mean_absolute_percentage_error(y_val_master, y_pred_temp)\n",
    "        \n",
    "        if test_mape < best_new_mape:\n",
    "            best_new_mape = test_mape\n",
    "            best_new_feature = feature\n",
    "\n",
    "    # After testing all remaining features, check if we found a winner\n",
    "    if best_new_feature is not None:\n",
    "        selected_features.append(best_new_feature)\n",
    "        remaining_features.remove(best_new_feature)\n",
    "        current_mape = best_new_mape\n",
    "        \n",
    "        print(f\"  Added '{best_new_feature}'. New Best MAPE: {current_mape * 100:.2f}%\")\n",
    "        \n",
    "    else:\n",
    "        # No more features improve the model\n",
    "        print(\"\\n--- Selection Complete ---\")\n",
    "        print(\"No more features improve the model.\")\n",
    "        break\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- FINAL MODEL (Master Selection) ---\")\n",
    "print(f\"Final Best MAPE: {current_mape * 100:.2f}%\")\n",
    "print(f\"Number of features selected: {len(selected_features)}\")\n",
    "print(\"\\nFinal list of selected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"Selector Model 1 MAPE (Dummies Only): 41.67%\")\n",
    "print(f\"FINAL MODEL MAPE (Master Universe): {current_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d48832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting FINAL 'Tedious' Forward Selection (v2) ---\n",
      "Testing combinations of 37 features...\n",
      "Starting MAPE (predicting the mean): 87.31%\n",
      "  Added 'weathersit_3'. New Best MAPE: 63.77%\n",
      "  Added 'windspeed'. New Best MAPE: 55.79%\n",
      "  Added 'mnth_cos'. New Best MAPE: 51.36%\n",
      "  Added 'mnth_11'. New Best MAPE: 48.21%\n",
      "  Added 'mnth_9'. New Best MAPE: 46.38%\n",
      "  Added 'mnth_12'. New Best MAPE: 44.83%\n",
      "  Added 'hum'. New Best MAPE: 43.29%\n",
      "  Added 'weekday_2'. New Best MAPE: 43.12%\n",
      "  Added 'holiday_1'. New Best MAPE: 43.03%\n",
      "  Added 'weekday_6'. New Best MAPE: 42.97%\n",
      "  Added 'mnth_cos_x_workingday'. New Best MAPE: 42.91%\n",
      "  Added 'mnth_8'. New Best MAPE: 42.88%\n",
      "  Added 'mnth_5'. New Best MAPE: 42.84%\n",
      "  Added 'weekday_4'. New Best MAPE: 42.83%\n",
      "  Added 'weekday_5'. New Best MAPE: 42.82%\n",
      "\n",
      "--- Selection Complete ---\n",
      "No more features improve the model.\n",
      "\n",
      "--- FINAL MODEL (Master Selection v2) ---\n",
      "Final Best MAPE: 42.82%\n",
      "Number of features selected: 15\n",
      "\n",
      "Final list of selected features:\n",
      "['weathersit_3', 'windspeed', 'mnth_cos', 'mnth_11', 'mnth_9', 'mnth_12', 'hum', 'weekday_2', 'holiday_1', 'weekday_6', 'mnth_cos_x_workingday', 'mnth_8', 'mnth_5', 'weekday_4', 'weekday_5']\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "Selector Model 1 MAPE (Dummies Only): 41.67%\n",
      "FINAL MODEL MAPE (Master Universe v2): 42.82%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import warnings\n",
    "\n",
    "# --- Re-run All Previous Steps to Build Master Data v2 ---\n",
    "\n",
    "# Suppress warnings for a clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and split\n",
    "day_df = pd.read_csv('./data/day.csv')\n",
    "d_train = day_df[day_df['yr'] == 0].copy()\n",
    "d_val = day_df[day_df['yr'] == 1].copy()\n",
    "target = 'cnt'\n",
    "\n",
    "# Step 1: Process Categorical Variables\n",
    "categorical_cols = ['season', 'mnth', 'weekday', 'holiday', 'weathersit']\n",
    "for col in categorical_cols:\n",
    "    d_train[col] = d_train[col].astype(str)\n",
    "    d_val[col] = d_val[col].astype(str)\n",
    "d_train_processed = pd.get_dummies(d_train, columns=categorical_cols, drop_first=True)\n",
    "d_val_processed = pd.get_dummies(d_val, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Step 2: Define base features\n",
    "continuous_cols = ['atemp', 'hum', 'windspeed']\n",
    "original_cols = list(d_train.columns)\n",
    "dummy_cols = [col for col in d_train_processed.columns if col not in original_cols and col != target]\n",
    "features = continuous_cols + dummy_cols\n",
    "\n",
    "# Step 8: Engineer \"Psychology\" Features\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "d_train_processed['workingday'] = d_train['workingday'].astype(int)\n",
    "d_val_processed['workingday'] = d_val['workingday'].astype(int)\n",
    "d_train_processed['atemp_x_workingday'] = d_train_processed['atemp'] * d_train_processed['workingday']\n",
    "d_val_processed['atemp_x_workingday'] = d_val_processed['atemp'] * d_val_processed['workingday']\n",
    "features_engineered = features + ['atemp_squared', 'workingday', 'atemp_x_workingday']\n",
    "\n",
    "# Step 10: Create Cyclical Features\n",
    "d_train_processed['mnth_int'] = d_train['mnth'].astype(int)\n",
    "d_val_processed['mnth_int'] = d_val['mnth'].astype(int)\n",
    "d_train_processed['weekday_int'] = d_train['weekday'].astype(int)\n",
    "d_val_processed['weekday_int'] = d_val['weekday'].astype(int)\n",
    "\n",
    "d_train_processed['mnth_sin'] = np.sin(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_train_processed['mnth_cos'] = np.cos(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_sin'] = np.sin(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_cos'] = np.cos(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "\n",
    "d_train_processed['weekday_sin'] = np.sin(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_train_processed['weekday_cos'] = np.cos(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_sin'] = np.sin(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_cos'] = np.cos(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "\n",
    "cyclical_features = ['mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n",
    "features_master_universe = features_engineered + cyclical_features\n",
    "\n",
    "# Step 11: Create Cyclical Interaction Features\n",
    "d_train_processed['mnth_sin_x_workingday'] = d_train_processed['mnth_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['mnth_cos_x_workingday'] = d_train_processed['mnth_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['mnth_sin_x_workingday'] = d_val_processed['mnth_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['mnth_cos_x_workingday'] = d_val_processed['mnth_cos'] * d_val_processed['workingday']\n",
    "\n",
    "d_train_processed['weekday_sin_x_workingday'] = d_train_processed['weekday_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['weekday_cos_x_workingday'] = d_train_processed['weekday_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['weekday_sin_x_workingday'] = d_val_processed['weekday_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['weekday_cos_x_workingday'] = d_val_processed['weekday_cos'] * d_val_processed['workingday']\n",
    "\n",
    "# --- THIS IS THE FINAL 37-FEATURE LIST ---\n",
    "new_interaction_features = [\n",
    "    'mnth_sin_x_workingday', 'mnth_cos_x_workingday',\n",
    "    'weekday_sin_x_workingday', 'weekday_cos_x_workingday'\n",
    "]\n",
    "features_master_universe_v2 = features_master_universe + new_interaction_features\n",
    "# ----------------------------------------\n",
    "\n",
    "# Create the final X and y for training and validation\n",
    "y_train_master = d_train_processed[target]\n",
    "y_val_master = d_val_processed[target]\n",
    "\n",
    "# --- Align all columns perfectly ---\n",
    "X_train_full, X_val_full = d_train_processed.align(d_val_processed, join='inner', axis=1, fill_value=0)\n",
    "X_train_master = X_train_full[features_master_universe_v2]\n",
    "X_val_master = X_val_full[features_master_universe_v2]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# --- STEP 12: The \"Tedious\" Selector on the \"Master Universe v2\" ---\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"--- Starting FINAL 'Tedious' Forward Selection (v2) ---\")\n",
    "print(f\"Testing combinations of {len(features_master_universe_v2)} features...\")\n",
    "\n",
    "# Start with a \"null\" model\n",
    "y_pred_null = np.full_like(y_val_master, y_train_master.mean())\n",
    "current_mape = mean_absolute_percentage_error(y_val_master, y_pred_null)\n",
    "print(f\"Starting MAPE (predicting the mean): {current_mape * 100:.2f}%\")\n",
    "\n",
    "selected_features = []\n",
    "remaining_features = features_master_universe_v2.copy()\n",
    "\n",
    "while True:\n",
    "    best_new_mape = current_mape\n",
    "    best_new_feature = None\n",
    "\n",
    "    # Test adding each remaining feature\n",
    "    for feature in remaining_features:\n",
    "        temp_features = selected_features + [feature]\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_master[temp_features], y_train_master)\n",
    "        \n",
    "        y_pred_temp = model.predict(X_val_master[temp_features])\n",
    "        \n",
    "        test_mape = mean_absolute_percentage_error(y_val_master, y_pred_temp)\n",
    "        \n",
    "        if test_mape < best_new_mape:\n",
    "            best_new_mape = test_mape\n",
    "            best_new_feature = feature\n",
    "\n",
    "    # After testing all remaining features, check if we found a winner\n",
    "    if best_new_feature is not None:\n",
    "        selected_features.append(best_new_feature)\n",
    "        remaining_features.remove(best_new_feature)\n",
    "        current_mape = best_new_mape\n",
    "        \n",
    "        print(f\"  Added '{best_new_feature}'. New Best MAPE: {current_mape * 100:.2f}%\")\n",
    "        \n",
    "    else:\n",
    "        # No more features improve the model\n",
    "        print(\"\\n--- Selection Complete ---\")\n",
    "        print(\"No more features improve the model.\")\n",
    "        break\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- FINAL MODEL (Master Selection v2) ---\")\n",
    "print(f\"Final Best MAPE: {current_mape * 100:.2f}%\")\n",
    "print(f\"Number of features selected: {len(selected_features)}\")\n",
    "print(\"\\nFinal list of selected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"Selector Model 1 MAPE (Dummies Only): 41.67%\")\n",
    "print(f\"FINAL MODEL MAPE (Master Universe v2): {current_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f790cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training 'Cheat' Neural Network (this may take a few moments) ---\n",
      "--- 'Cheat' Model Trained ---\n",
      "--- 'Boss' Linear Model Trained ---\n",
      "\n",
      "--- FINAL 'Cheat Code' Model ---\n",
      "Final Best MAPE: 63.14%\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "Selector Model 1 MAPE (Dummies Only): 41.67%\n",
      "FINAL 'Cheat' MODEL MAPE: 63.14%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "# --- Re-run All Previous Steps to Build Master Data v2 ---\n",
    "\n",
    "# Suppress warnings for a clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and split\n",
    "day_df = pd.read_csv('./data/day.csv')\n",
    "d_train = day_df[day_df['yr'] == 0].copy()\n",
    "d_val = day_df[day_df['yr'] == 1].copy()\n",
    "target = 'cnt'\n",
    "\n",
    "# (Steps 1, 2, 8, 10, 11 - combined for brevity)\n",
    "# --- Create the Master Universe v2 Data ---\n",
    "categorical_cols = ['season', 'mnth', 'weekday', 'holiday', 'weathersit']\n",
    "for col in categorical_cols:\n",
    "    d_train[col] = d_train[col].astype(str)\n",
    "    d_val[col] = d_val[col].astype(str)\n",
    "d_train_processed = pd.get_dummies(d_train, columns=categorical_cols, drop_first=True)\n",
    "d_val_processed = pd.get_dummies(d_val, columns=categorical_cols, drop_first=True)\n",
    "continuous_cols = ['atemp', 'hum', 'windspeed']\n",
    "original_cols = list(d_train.columns)\n",
    "dummy_cols = [col for col in d_train_processed.columns if col not in original_cols and col != target]\n",
    "features = continuous_cols + dummy_cols\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "d_train_processed['workingday'] = d_train['workingday'].astype(int)\n",
    "d_val_processed['workingday'] = d_val['workingday'].astype(int)\n",
    "d_train_processed['atemp_x_workingday'] = d_train_processed['atemp'] * d_train_processed['workingday']\n",
    "d_val_processed['atemp_x_workingday'] = d_val_processed['atemp'] * d_val_processed['workingday']\n",
    "features_engineered = features + ['atemp_squared', 'workingday', 'atemp_x_workingday']\n",
    "d_train_processed['mnth_int'] = d_train['mnth'].astype(int)\n",
    "d_val_processed['mnth_int'] = d_val['mnth'].astype(int)\n",
    "d_train_processed['weekday_int'] = d_train['weekday'].astype(int)\n",
    "d_val_processed['weekday_int'] = d_val['weekday'].astype(int)\n",
    "d_train_processed['mnth_sin'] = np.sin(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_train_processed['mnth_cos'] = np.cos(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_sin'] = np.sin(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_cos'] = np.cos(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_train_processed['weekday_sin'] = np.sin(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_train_processed['weekday_cos'] = np.cos(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_sin'] = np.sin(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_cos'] = np.cos(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "cyclical_features = ['mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n",
    "features_master_universe = features_engineered + cyclical_features\n",
    "d_train_processed['mnth_sin_x_workingday'] = d_train_processed['mnth_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['mnth_cos_x_workingday'] = d_train_processed['mnth_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['mnth_sin_x_workingday'] = d_val_processed['mnth_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['mnth_cos_x_workingday'] = d_val_processed['mnth_cos'] * d_val_processed['workingday']\n",
    "d_train_processed['weekday_sin_x_workingday'] = d_train_processed['weekday_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['weekday_cos_x_workingday'] = d_train_processed['weekday_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['weekday_sin_x_workingday'] = d_val_processed['weekday_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['weekday_cos_x_workingday'] = d_val_processed['weekday_cos'] * d_val_processed['workingday']\n",
    "new_interaction_features = ['mnth_sin_x_workingday', 'mnth_cos_x_workingday', 'weekday_sin_x_workingday', 'weekday_cos_x_workingday']\n",
    "features_master_universe_v2 = features_master_universe + new_interaction_features\n",
    "y_train_master = d_train_processed[target]\n",
    "y_val_master = d_val_processed[target]\n",
    "X_train_full, X_val_full = d_train_processed.align(d_val_processed, join='inner', axis=1, fill_value=0)\n",
    "X_train_master = X_train_full[features_master_universe_v2]\n",
    "X_val_master = X_val_full[features_master_universe_v2]\n",
    "# --- End of Data Prep ---\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# --- STEP 13: The \"Cheat Code\" (NN-Linear Stack) ---\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# 1. Scale the data for the Neural Network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_master)\n",
    "X_val_scaled = scaler.transform(X_val_master)\n",
    "\n",
    "# 2. Train the \"Cheat\" Model (Neural Network)\n",
    "print(\"--- Training 'Cheat' Neural Network (this may take a few moments) ---\")\n",
    "# (100, 50) = 2 hidden layers. max_iter=1000 to ensure it converges.\n",
    "# random_state=42 for reproducible \"cheat\"\n",
    "model_nn = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "model_nn.fit(X_train_scaled, y_train_master)\n",
    "print(\"--- 'Cheat' Model Trained ---\")\n",
    "\n",
    "# 3. Get the \"Cheat Feature\" (the NN's predictions)\n",
    "#    We get predictions for both train and val sets\n",
    "nn_feature_train = model_nn.predict(X_train_scaled)\n",
    "nn_feature_val = model_nn.predict(X_val_scaled)\n",
    "\n",
    "# 4. Create the \"Boss\" Model (Linear Regression)\n",
    "#    The *only* feature for this model is the \"cheat\" feature\n",
    "#    We use .reshape(-1, 1) to make it a 2D array for sklearn\n",
    "X_train_boss = nn_feature_train.reshape(-1, 1)\n",
    "X_val_boss = nn_feature_val.reshape(-1, 1)\n",
    "\n",
    "#    Now, create and fit the final LinearRegression\n",
    "model_boss = LinearRegression()\n",
    "model_boss.fit(X_train_boss, y_train_master)\n",
    "print(\"--- 'Boss' Linear Model Trained ---\")\n",
    "\n",
    "# 5. Test the \"Boss\" Model\n",
    "y_pred_boss = model_boss.predict(X_val_boss)\n",
    "final_mape = mean_absolute_percentage_error(y_val_master, y_pred_boss)\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- FINAL 'Cheat Code' Model ---\")\n",
    "print(f\"Final Best MAPE: {final_mape * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"Selector Model 1 MAPE (Dummies Only): 41.67%\")\n",
    "print(f\"FINAL 'Cheat' MODEL MAPE: {final_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2735a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready. Total features: 37\n",
      "X_train_scaled shape: (365, 37)\n",
      "X_val_scaled shape: (366, 37)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load and split ---\n",
    "day_df = pd.read_csv('./data/day.csv')\n",
    "d_train = day_df[day_df['yr'] == 0].copy()\n",
    "d_val = day_df[day_df['yr'] == 1].copy()\n",
    "target = 'cnt'\n",
    "\n",
    "# --- (Steps 1, 2, 8, 10, 11 - combined) ---\n",
    "# --- Create the Master Universe v2 Data ---\n",
    "categorical_cols = ['season', 'mnth', 'weekday', 'holiday', 'weathersit']\n",
    "for col in categorical_cols:\n",
    "    d_train[col] = d_train[col].astype(str)\n",
    "    d_val[col] = d_val[col].astype(str)\n",
    "d_train_processed = pd.get_dummies(d_train, columns=categorical_cols, drop_first=True)\n",
    "d_val_processed = pd.get_dummies(d_val, columns=categorical_cols, drop_first=True)\n",
    "continuous_cols = ['atemp', 'hum', 'windspeed']\n",
    "original_cols = list(d_train.columns)\n",
    "dummy_cols = [col for col in d_train_processed.columns if col not in original_cols and col != target]\n",
    "features = continuous_cols + dummy_cols\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "d_train_processed['workingday'] = d_train['workingday'].astype(int)\n",
    "d_val_processed['workingday'] = d_val['workingday'].astype(int)\n",
    "d_train_processed['atemp_x_workingday'] = d_train_processed['atemp'] * d_train_processed['workingday']\n",
    "d_val_processed['atemp_x_workingday'] = d_val_processed['atemp'] * d_val_processed['workingday']\n",
    "features_engineered = features + ['atemp_squared', 'workingday', 'atemp_x_workingday']\n",
    "d_train_processed['mnth_int'] = d_train['mnth'].astype(int)\n",
    "d_val_processed['mnth_int'] = d_val['mnth'].astype(int)\n",
    "d_train_processed['weekday_int'] = d_train['weekday'].astype(int)\n",
    "d_val_processed['weekday_int'] = d_val['weekday'].astype(int)\n",
    "d_train_processed['mnth_sin'] = np.sin(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_train_processed['mnth_cos'] = np.cos(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_sin'] = np.sin(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_cos'] = np.cos(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_train_processed['weekday_sin'] = np.sin(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_train_processed['weekday_cos'] = np.cos(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_sin'] = np.sin(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_cos'] = np.cos(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "cyclical_features = ['mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n",
    "features_master_universe = features_engineered + cyclical_features\n",
    "d_train_processed['mnth_sin_x_workingday'] = d_train_processed['mnth_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['mnth_cos_x_workingday'] = d_train_processed['mnth_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['mnth_sin_x_workingday'] = d_val_processed['mnth_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['mnth_cos_x_workingday'] = d_val_processed['mnth_cos'] * d_val_processed['workingday']\n",
    "d_train_processed['weekday_sin_x_workingday'] = d_train_processed['weekday_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['weekday_cos_x_workingday'] = d_train_processed['weekday_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['weekday_sin_x_workingday'] = d_val_processed['weekday_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['weekday_cos_x_workingday'] = d_val_processed['weekday_cos'] * d_val_processed['workingday']\n",
    "new_interaction_features = ['mnth_sin_x_workingday', 'mnth_cos_x_workingday', 'weekday_sin_x_workingday', 'weekday_cos_x_workingday']\n",
    "\n",
    "# --- This is the FINAL 37-FEATURE LIST ---\n",
    "features_master_universe_v2 = features_master_universe + new_interaction_features\n",
    "n_features = len(features_master_universe_v2) # Should be 37\n",
    "\n",
    "# --- Create final X and y ---\n",
    "y_train = d_train_processed[target]\n",
    "y_val = d_val_processed[target]\n",
    "\n",
    "# --- Align all columns perfectly ---\n",
    "X_train_full, X_val_full = d_train_processed.align(d_val_processed, join='inner', axis=1, fill_value=0)\n",
    "X_train = X_train_full[features_master_universe_v2]\n",
    "X_val = X_val_full[features_master_universe_v2]\n",
    "\n",
    "# --- Scale the data (CRITICAL for NNs) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"Data ready. Total features: {n_features}\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_val_scaled shape: {X_val_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca20653a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make sure you have tensorflow installed: pip install tensorflow\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Make sure you have tensorflow installed: pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- 1. Build the \"Cheat\" Model ---\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(64, input_dim=n_features, activation='relu'))\n",
    "model_nn.add(Dropout(0.2)) # Add dropout to prevent overfitting\n",
    "model_nn.add(Dense(32, activation='relu'))\n",
    "model_nn.add(Dense(1)) # Output layer: 1 neuron for 'cnt'\n",
    "\n",
    "# We can compile with 'mape' as the loss, or 'mse'\n",
    "# 'adam' is a good, standard optimizer\n",
    "model_nn.compile(optimizer='adam', loss='mape')\n",
    "\n",
    "print(\"--- Training 'Cheat' Neural Network ---\")\n",
    "\n",
    "# Use EarlyStopping: if the val_loss doesn't improve for 20 epochs, stop\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "# We train on 2011 data and validate on 2012 data\n",
    "history = model_nn.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    epochs=200, # Max epochs\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    callbacks=[early_stop],\n",
    "    batch_size=32,\n",
    "    verbose=1 # Set to 0 for less output\n",
    ")\n",
    "\n",
    "print(\"--- 'Cheat' Model Trained ---\")\n",
    "\n",
    "# --- 2. Get the \"Cheat Feature\" (the NN's predictions) ---\n",
    "nn_feature_train = model_nn.predict(X_train_scaled)\n",
    "nn_feature_val = model_nn.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d909d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/raul-fikratazizli/Library/Python/3.12/lib/python/site-packages (from tensorflow) (24.1)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/raul-fikratazizli/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.65.5)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Using cached keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.15.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (13.8.1)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Using cached optree-0.17.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (10.4.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/raul-fikratazizli/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl (200.5 MB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.15.1-cp312-cp312-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp312-cp312-macosx_10_13_universal2.whl (663 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.17.0-cp312-cp312-macosx_11_0_arm64.whl (351 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, markdown, h5py, google_pasta, gast, absl-py, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-language 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-secret-manager 2.21.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-dlp 3.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "wandb 0.19.0 requires protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0; sys_platform != \"linux\", but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-spanner 3.50.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-audit-log 0.3.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "proto-plus 1.24.0 requires protobuf<6.0.0dev,>=3.19.0, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-appengine-logging 1.4.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.72.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "apache-beam 2.60.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.60.0 requires protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-resource-manager 1.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-logging 3.11.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-api-core 2.20.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-recommendations-ai 0.10.13 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-pubsub 2.25.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-videointelligence 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-cloud-vision 3.7.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.3.1 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 markdown-3.10 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.33.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a4f1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready. Total features: 37\n",
      "X_train_scaled shape: (365, 37)\n",
      "X_val_scaled shape: (366, 37)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load and split ---\n",
    "day_df = pd.read_csv('./data/day.csv')\n",
    "d_train = day_df[day_df['yr'] == 0].copy()\n",
    "d_val = day_df[day_df['yr'] == 1].copy()\n",
    "target = 'cnt'\n",
    "\n",
    "# --- (Steps 1, 2, 8, 10, 11 - combined) ---\n",
    "# --- Create the Master Universe v2 Data ---\n",
    "categorical_cols = ['season', 'mnth', 'weekday', 'holiday', 'weathersit']\n",
    "for col in categorical_cols:\n",
    "    d_train[col] = d_train[col].astype(str)\n",
    "    d_val[col] = d_val[col].astype(str)\n",
    "d_train_processed = pd.get_dummies(d_train, columns=categorical_cols, drop_first=True)\n",
    "d_val_processed = pd.get_dummies(d_val, columns=categorical_cols, drop_first=True)\n",
    "continuous_cols = ['atemp', 'hum', 'windspeed']\n",
    "original_cols = list(d_train.columns)\n",
    "dummy_cols = [col for col in d_train_processed.columns if col not in original_cols and col != target]\n",
    "features = continuous_cols + dummy_cols\n",
    "d_train_processed['atemp_squared'] = d_train_processed['atemp']**2\n",
    "d_val_processed['atemp_squared'] = d_val_processed['atemp']**2\n",
    "d_train_processed['workingday'] = d_train['workingday'].astype(int)\n",
    "d_val_processed['workingday'] = d_val['workingday'].astype(int)\n",
    "d_train_processed['atemp_x_workingday'] = d_train_processed['atemp'] * d_train_processed['workingday']\n",
    "d_val_processed['atemp_x_workingday'] = d_val_processed['atemp'] * d_val_processed['workingday']\n",
    "features_engineered = features + ['atemp_squared', 'workingday', 'atemp_x_workingday']\n",
    "d_train_processed['mnth_int'] = d_train['mnth'].astype(int)\n",
    "d_val_processed['mnth_int'] = d_val['mnth'].astype(int)\n",
    "d_train_processed['weekday_int'] = d_train['weekday'].astype(int)\n",
    "d_val_processed['weekday_int'] = d_val['weekday'].astype(int)\n",
    "d_train_processed['mnth_sin'] = np.sin(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_train_processed['mnth_cos'] = np.cos(2 * np.pi * d_train_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_sin'] = np.sin(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_val_processed['mnth_cos'] = np.cos(2 * np.pi * d_val_processed['mnth_int'] / 12)\n",
    "d_train_processed['weekday_sin'] = np.sin(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_train_processed['weekday_cos'] = np.cos(2 * np.pi * d_train_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_sin'] = np.sin(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "d_val_processed['weekday_cos'] = np.cos(2 * np.pi * d_val_processed['weekday_int'] / 7)\n",
    "cyclical_features = ['mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n",
    "features_master_universe = features_engineered + cyclical_features\n",
    "d_train_processed['mnth_sin_x_workingday'] = d_train_processed['mnth_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['mnth_cos_x_workingday'] = d_train_processed['mnth_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['mnth_sin_x_workingday'] = d_val_processed['mnth_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['mnth_cos_x_workingday'] = d_val_processed['mnth_cos'] * d_val_processed['workingday']\n",
    "d_train_processed['weekday_sin_x_workingday'] = d_train_processed['weekday_sin'] * d_train_processed['workingday']\n",
    "d_train_processed['weekday_cos_x_workingday'] = d_train_processed['weekday_cos'] * d_train_processed['workingday']\n",
    "d_val_processed['weekday_sin_x_workingday'] = d_val_processed['weekday_sin'] * d_val_processed['workingday']\n",
    "d_val_processed['weekday_cos_x_workingday'] = d_val_processed['weekday_cos'] * d_val_processed['workingday']\n",
    "new_interaction_features = ['mnth_sin_x_workingday', 'mnth_cos_x_workingday', 'weekday_sin_x_workingday', 'weekday_cos_x_workingday']\n",
    "\n",
    "# --- This is the FINAL 37-FEATURE LIST ---\n",
    "features_master_universe_v2 = features_master_universe + new_interaction_features\n",
    "n_features = len(features_master_universe_v2) # Should be 37\n",
    "\n",
    "# --- Create final X and y ---\n",
    "# We'll keep these as numpy arrays for now\n",
    "y_train_np = d_train_processed[target].values\n",
    "y_val_np = d_val_processed[target].values\n",
    "\n",
    "# --- Align all columns perfectly ---\n",
    "X_train_full, X_val_full = d_train_processed.align(d_val_processed, join='inner', axis=1, fill_value=0)\n",
    "X_train = X_train_full[features_master_universe_v2]\n",
    "X_val = X_val_full[features_master_universe_v2]\n",
    "\n",
    "# --- Scale the data (CRITICAL for NNs) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"Data ready. Total features: {n_features}\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_val_scaled shape: {X_val_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a262ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training 'Cheat' Neural Network (PyTorch) ---\n",
      "Epoch 0/1000, Train Loss: 9245497.0000, Val Loss: 34403685.4783\n",
      "Epoch 10/1000, Train Loss: 9917321.0000, Val Loss: 29760427.9348\n",
      "Epoch 20/1000, Train Loss: 2813504.5000, Val Loss: 13557428.3261\n",
      "Epoch 30/1000, Train Loss: 610098.7500, Val Loss: 7990942.8152\n",
      "Epoch 40/1000, Train Loss: 665875.0000, Val Loss: 6768332.5761\n",
      "Epoch 50/1000, Train Loss: 562314.7500, Val Loss: 6386193.5000\n",
      "Epoch 60/1000, Train Loss: 330703.5000, Val Loss: 6142109.8152\n",
      "Epoch 70/1000, Train Loss: 495994.9688, Val Loss: 6044771.6793\n",
      "Epoch 80/1000, Train Loss: 467462.5000, Val Loss: 5954097.8207\n",
      "Epoch 90/1000, Train Loss: 233859.0000, Val Loss: 5799689.0571\n",
      "Epoch 100/1000, Train Loss: 157862.1406, Val Loss: 5812992.4130\n",
      "Epoch 110/1000, Train Loss: 240239.0156, Val Loss: 5640392.7935\n",
      "Epoch 120/1000, Train Loss: 391235.5625, Val Loss: 5795185.0353\n",
      "Epoch 130/1000, Train Loss: 414754.8750, Val Loss: 5689859.5679\n",
      "Epoch 140/1000, Train Loss: 484825.3125, Val Loss: 5623660.6223\n",
      "Epoch 150/1000, Train Loss: 212732.9688, Val Loss: 5671024.5788\n",
      "Epoch 160/1000, Train Loss: 687981.8750, Val Loss: 5650625.7473\n",
      "Epoch 170/1000, Train Loss: 199245.0312, Val Loss: 5665814.4266\n",
      "Epoch 180/1000, Train Loss: 607621.4375, Val Loss: 5564272.7772\n",
      "Epoch 190/1000, Train Loss: 411659.0312, Val Loss: 5589350.6223\n",
      "Epoch 200/1000, Train Loss: 183794.3594, Val Loss: 5643251.2120\n",
      "Epoch 210/1000, Train Loss: 361159.0000, Val Loss: 5523794.9103\n",
      "Epoch 220/1000, Train Loss: 396351.2188, Val Loss: 5445956.3370\n",
      "Epoch 230/1000, Train Loss: 129441.7500, Val Loss: 5448199.3804\n",
      "Epoch 240/1000, Train Loss: 205007.9219, Val Loss: 5426379.4973\n",
      "Epoch 250/1000, Train Loss: 346752.1875, Val Loss: 5513597.2337\n",
      "Epoch 260/1000, Train Loss: 127028.0391, Val Loss: 5431457.1739\n",
      "Epoch 270/1000, Train Loss: 147045.6250, Val Loss: 5534655.5734\n",
      "Epoch 280/1000, Train Loss: 219067.2656, Val Loss: 5336302.1495\n",
      "Epoch 290/1000, Train Loss: 146964.7969, Val Loss: 5542379.3288\n",
      "Epoch 300/1000, Train Loss: 352048.9375, Val Loss: 5502484.1875\n",
      "Epoch 310/1000, Train Loss: 235093.4062, Val Loss: 5383266.7663\n",
      "Epoch 320/1000, Train Loss: 154083.8594, Val Loss: 5367592.5788\n",
      "Epoch 330/1000, Train Loss: 111180.0000, Val Loss: 5281628.7935\n",
      "Epoch 340/1000, Train Loss: 345531.8750, Val Loss: 5345675.2745\n",
      "Epoch 350/1000, Train Loss: 166292.1875, Val Loss: 5228811.9076\n",
      "Epoch 360/1000, Train Loss: 286514.2500, Val Loss: 5178477.1386\n",
      "Epoch 370/1000, Train Loss: 324056.3750, Val Loss: 5368057.7582\n",
      "Epoch 380/1000, Train Loss: 496695.5312, Val Loss: 5282725.7120\n",
      "Epoch 390/1000, Train Loss: 166703.1719, Val Loss: 5299175.8342\n",
      "Epoch 400/1000, Train Loss: 548442.6250, Val Loss: 5172752.2554\n",
      "Epoch 410/1000, Train Loss: 196678.6719, Val Loss: 5194911.6005\n",
      "Epoch 420/1000, Train Loss: 226703.9688, Val Loss: 5301989.2011\n",
      "Epoch 430/1000, Train Loss: 237430.7812, Val Loss: 5173631.9946\n",
      "Epoch 440/1000, Train Loss: 205818.1875, Val Loss: 5288856.3750\n",
      "Epoch 450/1000, Train Loss: 291034.7812, Val Loss: 5152846.1766\n",
      "Epoch 460/1000, Train Loss: 342952.7188, Val Loss: 5242013.3560\n",
      "Epoch 470/1000, Train Loss: 63188.8281, Val Loss: 5111590.4266\n",
      "Epoch 480/1000, Train Loss: 486090.0312, Val Loss: 5182560.1766\n",
      "Epoch 490/1000, Train Loss: 177716.8906, Val Loss: 5271102.0598\n",
      "Epoch 500/1000, Train Loss: 123519.1953, Val Loss: 5130397.7717\n",
      "Epoch 510/1000, Train Loss: 182971.3594, Val Loss: 5149446.1576\n",
      "Epoch 520/1000, Train Loss: 260388.4062, Val Loss: 5129627.2337\n",
      "Epoch 530/1000, Train Loss: 290106.0312, Val Loss: 5216084.1549\n",
      "Epoch 540/1000, Train Loss: 132031.9844, Val Loss: 5296987.9457\n",
      "Epoch 550/1000, Train Loss: 171466.5156, Val Loss: 5143098.3342\n",
      "Epoch 560/1000, Train Loss: 215464.6094, Val Loss: 5235621.8668\n",
      "Epoch 570/1000, Train Loss: 149760.9531, Val Loss: 5095569.0543\n",
      "Epoch 580/1000, Train Loss: 123048.5469, Val Loss: 5122419.5489\n",
      "Epoch 590/1000, Train Loss: 160078.1406, Val Loss: 5175964.8641\n",
      "Epoch 600/1000, Train Loss: 163275.7344, Val Loss: 5117392.7147\n",
      "Epoch 610/1000, Train Loss: 131918.7500, Val Loss: 5128487.7364\n",
      "Epoch 620/1000, Train Loss: 128596.7656, Val Loss: 5027712.9647\n",
      "Epoch 630/1000, Train Loss: 350731.1562, Val Loss: 5059354.3641\n",
      "Epoch 640/1000, Train Loss: 192041.8906, Val Loss: 5173613.9728\n",
      "Epoch 650/1000, Train Loss: 242038.6094, Val Loss: 5130016.2772\n",
      "Epoch 660/1000, Train Loss: 193336.8906, Val Loss: 5166396.6902\n",
      "Epoch 670/1000, Train Loss: 266834.3125, Val Loss: 5077007.6902\n",
      "Epoch 680/1000, Train Loss: 178726.4844, Val Loss: 5063596.7935\n",
      "Epoch 690/1000, Train Loss: 470712.6250, Val Loss: 5093819.6033\n",
      "Epoch 700/1000, Train Loss: 236498.7656, Val Loss: 5201911.2147\n",
      "Epoch 710/1000, Train Loss: 165656.1719, Val Loss: 5143834.1386\n",
      "Epoch 720/1000, Train Loss: 257174.5938, Val Loss: 4959023.4429\n",
      "Epoch 730/1000, Train Loss: 192573.6719, Val Loss: 5117193.6114\n",
      "Epoch 740/1000, Train Loss: 178973.1562, Val Loss: 4959666.8424\n",
      "Epoch 750/1000, Train Loss: 336266.3750, Val Loss: 5048931.9185\n",
      "Epoch 760/1000, Train Loss: 210409.8281, Val Loss: 5095136.3560\n",
      "Epoch 770/1000, Train Loss: 175669.9688, Val Loss: 5037949.2255\n",
      "Epoch 780/1000, Train Loss: 120919.1250, Val Loss: 5070292.8832\n",
      "Epoch 790/1000, Train Loss: 255173.5938, Val Loss: 4953634.0489\n",
      "Epoch 800/1000, Train Loss: 251511.0625, Val Loss: 5041232.7228\n",
      "Epoch 810/1000, Train Loss: 71026.7812, Val Loss: 5048497.7337\n",
      "Epoch 820/1000, Train Loss: 290662.5625, Val Loss: 5059612.8560\n",
      "Epoch 830/1000, Train Loss: 282904.3438, Val Loss: 5012137.9158\n",
      "Epoch 840/1000, Train Loss: 149799.9844, Val Loss: 5088984.0625\n",
      "Epoch 850/1000, Train Loss: 185014.2812, Val Loss: 5006456.1549\n",
      "Epoch 860/1000, Train Loss: 244621.1875, Val Loss: 5038480.5462\n",
      "Epoch 870/1000, Train Loss: 164967.8594, Val Loss: 5015662.1332\n",
      "Epoch 880/1000, Train Loss: 266014.7500, Val Loss: 5040552.4185\n",
      "Epoch 890/1000, Train Loss: 247416.7812, Val Loss: 4960548.7120\n",
      "Epoch 900/1000, Train Loss: 90831.1641, Val Loss: 4989468.2582\n",
      "Epoch 910/1000, Train Loss: 101897.1328, Val Loss: 4962567.1658\n",
      "Epoch 920/1000, Train Loss: 251869.0312, Val Loss: 5100298.2853\n",
      "Epoch 930/1000, Train Loss: 201642.0938, Val Loss: 4969763.9185\n",
      "Epoch 940/1000, Train Loss: 269252.7188, Val Loss: 4942428.6005\n",
      "Epoch 950/1000, Train Loss: 69738.4688, Val Loss: 4923476.2690\n",
      "Epoch 960/1000, Train Loss: 242274.6094, Val Loss: 4985066.7582\n",
      "Epoch 970/1000, Train Loss: 210574.3438, Val Loss: 5033890.9973\n",
      "Epoch 980/1000, Train Loss: 146561.4062, Val Loss: 4944509.8859\n",
      "Epoch 990/1000, Train Loss: 313289.1875, Val Loss: 4992920.2772\n",
      "--- 'Cheat' Model Trained ---\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have pytorch installed: pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- 1. Convert data to PyTorch Tensors ---\n",
    "# We need to explicitly set the type to float32\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_np.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# --- 2. Create DataLoaders for batching ---\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- 3. Define the \"Cheat\" Model Architecture ---\n",
    "# This matches the Keras model (37 -> 64 -> 32 -> 1)\n",
    "class CheatModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CheatModel, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_features, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "model_nn = CheatModel(n_features=n_features)\n",
    "\n",
    "# --- 4. Define Loss, Optimizer, and Early Stopping ---\n",
    "# We'll use MSELoss (L2) as our loss function\n",
    "criterion = nn.MSELoss()\n",
    "# 'Adam' is a good, standard optimizer\n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=0.001)\n",
    "\n",
    "# Manual early stopping parameters\n",
    "n_epochs = 1000\n",
    "patience = 200\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"--- Training 'Cheat' Neural Network (PyTorch) ---\")\n",
    "\n",
    "# --- 5. The Training Loop ---\n",
    "for epoch in range(n_epochs):\n",
    "    model_nn.train() # Set model to training mode\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        y_pred = model_nn(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad() # Clear old gradients\n",
    "        loss.backward() # Calculate new gradients\n",
    "        optimizer.step() # Update model weights\n",
    "    \n",
    "    # --- Validation ---\n",
    "    model_nn.eval() # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            y_val_pred = model_nn(X_val_batch)\n",
    "            v_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_loss += v_loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader) # Get average validation loss\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the state of the best model\n",
    "        best_model_state = model_nn.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "# --- 6. Load the best model weights\n",
    "if best_model_state:\n",
    "    model_nn.load_state_dict(best_model_state)\n",
    "\n",
    "print(\"--- 'Cheat' Model Trained ---\")\n",
    "\n",
    "# --- 7. Get the \"Cheat Feature\" (the NN's predictions) ---\n",
    "model_nn.eval()\n",
    "with torch.no_grad():\n",
    "    # Get predictions for both train and val sets\n",
    "    nn_feature_train_tensor = model_nn(X_train_tensor)\n",
    "    nn_feature_val_tensor = model_nn(X_val_tensor)\n",
    "\n",
    "# Convert back to numpy\n",
    "nn_feature_train = nn_feature_train_tensor.detach().numpy()\n",
    "nn_feature_val = nn_feature_val_tensor.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22bca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'Boss' Linear Model Trained ---\n",
      "\n",
      "--- FINAL 'Cheat Code' Model ---\n",
      "Final Best MAPE: 62.74%\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "Selector Model 1 MAPE (Dummies Only): 41.67%\n",
      "FINAL 'Cheat' MODEL MAPE: 62.74%\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create the \"Boss\" Model ---\n",
    "# The *only* feature is the \"cheat\" feature\n",
    "# We use .reshape(-1, 1) to make it a 2D array for sklearn\n",
    "X_train_boss = nn_feature_train.reshape(-1, 1)\n",
    "X_val_boss = nn_feature_val.reshape(-1, 1)\n",
    "\n",
    "# Create and fit the final LinearRegression\n",
    "model_boss = LinearRegression()\n",
    "model_boss.fit(X_train_boss, y_train_np)\n",
    "\n",
    "print(\"--- 'Boss' Linear Model Trained ---\")\n",
    "\n",
    "# --- 4. Test the \"Boss\" Model ---\n",
    "y_pred_boss = model_boss.predict(X_val_boss)\n",
    "# Use y_val_np (the original numpy array) for the true values\n",
    "final_mape = mean_absolute_percentage_error(y_val_np, y_pred_boss)\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- FINAL 'Cheat Code' Model ---\")\n",
    "print(f\"Final Best MAPE: {final_mape * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"Selector Model 1 MAPE (Dummies Only): 41.67%\")\n",
    "print(f\"FINAL 'Cheat' MODEL MAPE: {final_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7414d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training 'Cheat' Neural Network (FAE Edition) ---\n",
      "Epoch 0/200, Train Loss: 13715786.0000, Val Loss: 33339850.2500\n",
      "Epoch 10/200, Train Loss: 14189424.0000, Val Loss: 32723969.0833\n",
      "Epoch 20/200, Train Loss: 10907225.0000, Val Loss: 28204459.7083\n",
      "Epoch 30/200, Train Loss: 5131220.0000, Val Loss: 19097694.1667\n",
      "Epoch 40/200, Train Loss: 1367250.6250, Val Loss: 12030825.0417\n",
      "Epoch 50/200, Train Loss: 892834.3750, Val Loss: 8745605.5000\n",
      "Epoch 60/200, Train Loss: 894373.0000, Val Loss: 7345722.3229\n",
      "Epoch 70/200, Train Loss: 452885.4375, Val Loss: 6689895.0938\n",
      "Epoch 80/200, Train Loss: 526979.5625, Val Loss: 6361564.9479\n",
      "Epoch 90/200, Train Loss: 348940.1875, Val Loss: 6165518.9688\n",
      "Epoch 100/200, Train Loss: 695391.5625, Val Loss: 6045296.5938\n",
      "Epoch 110/200, Train Loss: 228648.0625, Val Loss: 5971394.0208\n",
      "Epoch 120/200, Train Loss: 449909.6875, Val Loss: 5845096.2604\n",
      "Epoch 130/200, Train Loss: 214944.8594, Val Loss: 5855703.4896\n",
      "Epoch 140/200, Train Loss: 459877.0000, Val Loss: 5708887.1771\n",
      "Epoch 150/200, Train Loss: 220024.5625, Val Loss: 5770832.9062\n",
      "Epoch 160/200, Train Loss: 1043494.3750, Val Loss: 5655551.6094\n",
      "Epoch 170/200, Train Loss: 274284.5625, Val Loss: 5671702.2344\n",
      "Epoch 180/200, Train Loss: 370928.4688, Val Loss: 5660113.9688\n",
      "Epoch 190/200, Train Loss: 534543.0625, Val Loss: 5649250.2396\n",
      "--- 'Cheat' Model Trained ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Convert data to PyTorch Tensors ---\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_np.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# --- 2. Create DataLoaders for batching ---\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- 3. Define the \"Cheat\" Model Architecture ---\n",
    "class CheatModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CheatModel, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_features, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "model_nn = CheatModel(n_features=n_features) # n_features will be 8\n",
    "\n",
    "# --- 4. Define Loss, Optimizer, and Early Stopping ---\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_nn.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 200\n",
    "patience = 20\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"--- Training 'Cheat' Neural Network (FAE Edition) ---\")\n",
    "\n",
    "# --- 5. The Training Loop ---\n",
    "for epoch in range(n_epochs):\n",
    "    model_nn.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model_nn(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # --- Validation ---\n",
    "    model_nn.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            y_val_pred = model_nn(X_val_batch)\n",
    "            v_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_loss += v_loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model_nn.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "# --- 6. Load the best model weights\n",
    "if best_model_state:\n",
    "    model_nn.load_state_dict(best_model_state)\n",
    "print(\"--- 'Cheat' Model Trained ---\")\n",
    "\n",
    "# --- 7. Get the \"Cheat Feature\" (the NN's predictions) ---\n",
    "model_nn.eval()\n",
    "with torch.no_grad():\n",
    "    nn_feature_train = model_nn(X_train_tensor).detach().numpy()\n",
    "    nn_feature_val = model_nn(X_val_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448d436f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'Boss' Linear Model Trained ---\n",
      "\n",
      "--- FINAL 'FAE Cheat Code' Model ---\n",
      "Final Best MAPE: 62.47%\n",
      "\n",
      "--- Comparison ---\n",
      "Baseline Model MAPE (One Big Model): 63.62%\n",
      "Selector Model 1 MAPE (Dummies Only, 14 features): 41.67%\n",
      "FINAL 'FAE' MODEL MAPE (8 features + NN): 62.47%\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create the \"Boss\" Model ---\n",
    "X_train_boss = nn_feature_train.reshape(-1, 1)\n",
    "X_val_boss = nn_feature_val.reshape(-1, 1)\n",
    "\n",
    "model_boss = LinearRegression()\n",
    "model_boss.fit(X_train_boss, y_train_np)\n",
    "print(\"--- 'Boss' Linear Model Trained ---\")\n",
    "\n",
    "# --- 4. Test the \"Boss\" Model ---\n",
    "y_pred_boss = model_boss.predict(X_val_boss)\n",
    "final_mape = mean_absolute_percentage_error(y_val_np, y_pred_boss)\n",
    "\n",
    "# --- Check Your Work ---\n",
    "print(\"\\n--- FINAL 'FAE Cheat Code' Model ---\")\n",
    "print(f\"Final Best MAPE: {final_mape * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Baseline Model MAPE (One Big Model): 63.62%\")\n",
    "print(f\"Selector Model 1 MAPE (Dummies Only, 14 features): 41.67%\")\n",
    "print(f\"FINAL 'FAE' MODEL MAPE (8 features + NN): {final_mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f5443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
